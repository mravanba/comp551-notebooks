{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "colab": {
   "name": "CurseOfDimensionality.ipynb",
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I2M516HsUw--"
   },
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mravanba/comp551-notebooks/blob/master/CurseOfDimensionality.ipynb)\n",
    "\n",
    "\n",
    "# Curse of Dimensionality\n",
    "\n",
    "Some learning algorithms use a distance function to measure the similarity or dissimilarity of instances. Having many features corresponds to the instance \"living\" in a high-dimensional space. High dimensional data introduce difficulties, so called curse of dimensionality. However, by making some strong assumption about the data machine learning methods overcome these difficulties. First lets see the problems associated with high-dimensional data:\n",
    "\n",
    "1. To \"fill\" a high dimensional space we need exponentially more samples.\n",
    "- in one dimension, it is enough to have 9 labeled examples in the range (0,10), to make sure any new observation will be at a distance of `1` to our training data.\n",
    "- in two dimensions, we need more than $9 \\times 9$ labeled examples to satisfy the same condition.\n",
    "- the number of required samples is $\\mathcal{O}(9^D)$, where $D$ is the dimension.\n",
    "\n",
    "2. In high dimension, it becomes harder to identify close neighbours. Let us demonstrate this with an example. We will generate random instances in various dimensions and measure the pairwise distance between these examples. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Xqc-SKkEUw-_"
   },
   "source": "import numpy as np\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom IPython.core.debugger import set_trace\n# Set random seed for reproducibility\nnp.random.seed(1234)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "jWOFDAhcUw_C",
    "outputId": "caea7a75-83b2-4d7b-e39d-2da6a9e3f066",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    }
   },
   "source": "N = 1000  # Number of randomly generated points\nDs = [1, 2, 8, 32, 128, 784]  # Different dimensions to test\n\n# Create subplots for each dimension\nfig, axes = plt.subplots(ncols=len(Ds), nrows=1, constrained_layout=True, figsize=(len(Ds)*3, 3))\n\nfor i, D in enumerate(Ds):\n    # Generate N random points in D-dimensional space\n    # x has shape (N, D) - each row is a D-dimensional point\n    # Points are uniformly distributed in [0, 1]^D\n    x = np.random.rand(N, D)\n    \n    # Compute pairwise Euclidean distances using broadcasting\n    # x[None,:,:] has shape (1, N, D) - adds a dimension at the front\n    # x[:,None,:] has shape (N, 1, D) - adds a dimension in the middle\n    # Broadcasting: (1, N, D) - (N, 1, D) = (N, N, D)\n    # Result: dist[i,j] = distance between point i and point j\n    # Subtraction creates all pairwise differences\n    # Square, sum over features (axis -1), then square root\n    dist = np.sqrt(np.sum((x[None, :, :] - x[:, None, :])**2, -1))\n    \n    # Plot histogram of all pairwise distances\n    # dist.ravel() flattens (N, N) matrix to 1D array of all distances\n    axes[i].hist(dist.ravel(), bins=100)\n    axes[i].set_xlabel(\"pairwise distance\")\n    axes[i].set_ylabel(\"frequency\")\n    axes[i].set_title(f'{D} dimensions')\n\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iP-SgCcXUw_G"
   },
   "source": [
    "It is evident from this plot that, very unintuitively, in high dimensions, most points in the space have similar distances with each other!\n",
    "If our datasets had a similar behaviour this could undermine learning in high-dimensions:\n",
    "if all instances are more or less similar to each other on what basis can we label them differently?\n",
    "Let's create similar plots for the `MNIST` dataset. For each dimension `D` we randomly select subset of dimensions from the `28 x 28 = 784` dimensions, and use them to measure the pairwise distance between examples."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "enLmb0laUw_H"
   },
   "source": "# Load the MNIST dataset\n# 70,000 images of handwritten digits (0-9)\n# Each image is 28x28 pixels = 784 features\nfrom sklearn.datasets import fetch_openml\nx_org, y = fetch_openml('mnist_784', version=1, return_X_y=True)",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GKf-4NdlUw_L"
   },
   "source": [
    "Lets see what the data looks like"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Khn_CwAzUw_L",
    "outputId": "abb1bd16-9298-45a4-d5db-eb69c64055d0",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 100
    }
   },
   "source": "from mpl_toolkits.axes_grid1 import ImageGrid\n\ndef plot_digits(data):\n    \"\"\"\n    Plot a grid of digit images from MNIST.\n    \n    Parameters:\n    data: array of flattened images, shape (num_images, 784)\n    \"\"\"\n    num_plots = data.shape[0]\n    \n    fig = plt.figure(figsize=(num_plots, 10.*num_plots))\n    \n    # Create a grid layout for displaying multiple images\n    # nrows_ncols=(1, num_plots) means 1 row, num_plots columns\n    grid = ImageGrid(fig, 111, nrows_ncols=(1, num_plots), axes_pad=0.1)\n    \n    for i in range(num_plots):\n        # Reshape flattened image (784,) back to 2D (28, 28) for display\n        grid[i].imshow(data[i].reshape((28, 28)))\n    \n    plt.show()\n\n# Display the first 20 digits from the dataset\nplot_digits(x_org[:20])",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ouoMyzYGUw_N"
   },
   "source": [
    "Now let's do the same measurement of pairwise distance, this time between instances in our dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "dgpZZCXGUw_O",
    "outputId": "535017c0-9ed7-4bef-f360-87b35522fe50",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    }
   },
   "source": "N = 1000  # Number of examples to use (for faster computation)\n\n# Flatten and prepare the data\n# x_org[:N] takes first N images\n# reshape with -1 infers the dimension: (N, 28, 28) -> (N, 784)\nx = np.reshape(x_org[:N], (N, -1))\n\n# Normalize pixel values to [0, 1] range\n# Original values are in [0, 255]\nx = x / np.max(x)\n\n# Test with different numbers of dimensions\nDs = [32, 128, 784]\n\nfig, axes = plt.subplots(ncols=len(Ds), nrows=1, constrained_layout=True, figsize=(len(Ds)*3, 3))\n\n# Randomly permute dimension indices to select random subsets of features\n# This creates a random ordering of indices [0, 1, 2, ..., 783]\ndim_inds = np.random.permutation(x.shape[1])\n\nfor i, D in enumerate(Ds):\n    # Select D random dimensions from the data\n    # dim_inds[:D] gives the first D indices from the permutation\n    # x[:, dim_inds[:D]] selects those D columns, result is (N, D)\n    x_D = x[:, dim_inds[:D]]\n    \n    # Compute pairwise Euclidean distances\n    # Same broadcasting trick as before:\n    # x_D[None,:,:] is (1, N, D)\n    # x_D[:,None,:] is (N, 1, D)\n    # Result dist is (N, N) where dist[i,j] = distance between sample i and j\n    dist = np.sqrt(np.sum((x_D[None, :, :] - x_D[:, None, :])**2, -1))\n    \n    # Plot histogram of all pairwise distances\n    axes[i].hist(dist.ravel(), bins=100)\n    axes[i].set_xlabel(\"pairwise distance\")\n    axes[i].set_ylabel(\"frequency\")\n    axes[i].set_title(f'{D} dimensions')\n\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RNOFaLQGUw_Q"
   },
   "source": [
    "This difference between real-world data and random data is explained by the **manifold hypothesis**. This hypothesis postulates that real-world data often reside close to high-dimensional *manifold*. Dimensionality reduction methods (aka manifold learning) try to estimate these low-dimensional encoding of the data. However, the point we wanted to show here is that because of this special behaviour, methods such as KNN continue to work with high-dimensional data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gqu4lYsNUw_R"
   },
   "source": [
    "## Another demonstration\n",
    "We put one hypercube inside another with unit length, such that they have one common corner. We then place a large number of points regularly inside the larger hypercube.\n",
    "Here again we change the dimension of the hypercubes and plot the portion of the points in the large hypercube that are also in the small hypercube (think of this as the portion of their volume) as we change the length of the side of the smaller hypercube."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "LhFyH63NUw_R",
    "outputId": "66be22c9-74ef-48a1-ffd5-ab875121b2a5",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 279
    }
   },
   "source": "Ds = [1, 2, 8, 128, 512, 784]  # Different dimensions to test\nN = 10000  # Number of random points to generate\ngrid_size = 1000  # Number of different cube sizes to test\n\n# Generate grid of cube side lengths from 0 to 1\ngrid_points = np.linspace(0, 1, grid_size, endpoint=True)\n\n# Store results: result[dimension_index, cube_size_index]\nresult = np.zeros((len(Ds), grid_size))\n\nfor j, D in enumerate(Ds):\n    # Generate N random points in D-dimensional unit hypercube [0,1]^D\n    # x has shape (N, D)\n    x = np.random.rand(N, D)\n    \n    for i, g in enumerate(grid_points):\n        # Count points where ALL coordinates are less than g\n        # np.all(x < g, axis=1) checks if all D coordinates of each point are < g\n        # This gives shape (N,) boolean array\n        # Points satisfying this condition are inside the smaller cube [0,g]^D\n        # Sum gives count, divide by N to get proportion\n        result[j, i] = np.sum(np.all(x < g, axis=1)) / N\n    \n    # Plot the proportion of points inside as a function of cube side length\n    plt.plot(grid_points, result[j, :], label=f'D={D}')\n\nplt.xlabel('side of the cube')\nplt.ylabel('portion of the points inside')\nplt.legend()\nplt.title('Hypercube Volume: Curse of Dimensionality')\nplt.grid(True, alpha=0.3)\nplt.show()",
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "64XqhoipUw_U"
   },
   "source": [
    "that is to get 1% of the points to be inside the inner cube we need to have a side of length .993! This is another expression of the idea that in high dimensions, *the mass is mostly at the corners!*"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1UyvPGg2Uw_U",
    "outputId": "f58c0381-d060-4b7a-9792-2d1f995bbed2",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 296
    }
   },
   "source": "# Find the cube side length needed to capture 1% of the points\n# For each dimension, find where result is closest to 0.01\n# np.argmin(np.abs(result - .01), axis=1) finds the index along axis 1 (cube sizes)\n# Divide by grid_size to convert index back to actual side length\nside_length = np.argmin(np.abs(result - .01), axis=1) / grid_size\n\n# Plot how required side length changes with dimension\nplt.plot(Ds, side_length, marker='o', linewidth=2, markersize=8)\nplt.xlabel('dimension')\nplt.ylabel('side of the sub-cube with 1% of the data')\nplt.title('Required Cube Size for 1% of Points')\nplt.grid(True, alpha=0.3)\nplt.show()\n\nprint(f'To get 1% of points inside the inner cube for D=784 we need the side of the cube to be of length {side_length[-1]:.3f}')\nprint(f'This means the cube must have {side_length[-1]*100:.1f}% of the maximum side length!')",
   "execution_count": null,
   "outputs": []
  }
 ]
}