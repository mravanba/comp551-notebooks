{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mravanba/comp551-notebooks/blob/master/ConvNets.ipynb)\n",
    "\n",
    "# Convolutional Neural Networks for Images\n",
    "\n",
    "Our goal is to implement a Convolutional Neural Network (ConvNet) for image classification using PyTorch. We'll use the MNIST dataset, which contains 70,000 grayscale images of handwritten digits (0-9). To make training faster on CPU, we'll downscale images from 28×28 to 14×14 pixels.\n",
    "\n",
    "ConvNets are designed to exploit the spatial structure in images. Unlike fully connected networks that treat each pixel independently, ConvNets use:\n",
    "- **Convolutional layers**: Apply learned filters to detect local patterns (edges, textures, etc.)\n",
    "- **Pooling layers**: Downsample spatial dimensions while preserving important features\n",
    "- **Spatial hierarchy**: Early layers detect simple features, deeper layers combine them into complex patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#%matplotlib notebook\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# For loading CIFAR-10 dataset\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "# This ensures results are consistent across runs\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "# We'll default to CPU but provide the option to use GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading MNIST Dataset\n",
    "\n",
    "MNIST contains 70,000 28×28 grayscale images of handwritten digits:\n",
    "- **Training set**: 60,000 images\n",
    "- **Test set**: 10,000 images\n",
    "- **Classes**: Digits 0-9 (10 classes)\n",
    "\n",
    "We'll apply transformations to:\n",
    "1. Resize from 28×28 to 14×14 (faster training on CPU)\n",
    "2. Convert PIL images to PyTorch tensors\n",
    "3. Normalize pixel values to have mean 0 and std 1 (improves training stability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 60000\n",
      "Test samples: 10000\n",
      "Number of classes: 10\n",
      "Image shape after transform: torch.Size([1, 14, 14])\n"
     ]
    }
   ],
   "source": [
    "# Define transformations for the dataset\n",
    "# Compose chains multiple transformations together\n",
    "transform = transforms.Compose([\n",
    "    # Resize images from 28x28 to 14x14 for faster CPU training\n",
    "    # Bilinear interpolation preserves smoothness\n",
    "    transforms.Resize((14, 14)),\n",
    "    \n",
    "    # Convert PIL Image to PyTorch tensor\n",
    "    # This changes the shape from (H, W) to (1, H, W) for grayscale\n",
    "    # and scales pixel values from [0, 255] to [0, 1]\n",
    "    transforms.ToTensor(),\n",
    "    \n",
    "    # Normalize with MNIST mean and std (computed across dataset)\n",
    "    # This centers data around 0 and scales to unit variance\n",
    "    # mean=0.5, std=0.5 scales roughly to [-1, 1] range\n",
    "    # For grayscale, we use single values instead of [0.5, 0.5, 0.5]\n",
    "    transforms.Normalize(mean=(0.5,), std=(0.5,))\n",
    "])\n",
    "\n",
    "# Download and load training dataset\n",
    "# root='./data': where to save downloaded data\n",
    "# train=True: get training split\n",
    "# download=True: download if not already present\n",
    "trainset = torchvision.datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=True,\n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Download and load test dataset\n",
    "testset = torchvision.datasets.MNIST(\n",
    "    root='./data', \n",
    "    train=False,\n",
    "    download=True, \n",
    "    transform=transform\n",
    ")\n",
    "\n",
    "# Create data loaders for batch processing\n",
    "# DataLoader handles batching, shuffling, and parallel loading\n",
    "# batch_size=64: process 64 images at a time (balance between speed and memory)\n",
    "# shuffle=True: randomize order each epoch (important for training)\n",
    "# num_workers=2: use 2 parallel processes to load data (speeds up loading)\n",
    "trainloader = torch.utils.data.DataLoader(\n",
    "    trainset, \n",
    "    batch_size=64, \n",
    "    shuffle=True, \n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "testloader = torch.utils.data.DataLoader(\n",
    "    testset, \n",
    "    batch_size=64, \n",
    "    shuffle=False,  # No need to shuffle test data\n",
    "    num_workers=2\n",
    ")\n",
    "\n",
    "# MNIST class labels (digits)\n",
    "classes = tuple(str(i) for i in range(10))\n",
    "\n",
    "print(f'Training samples: {len(trainset)}')\n",
    "print(f'Test samples: {len(testset)}')\n",
    "print(f'Number of classes: {len(classes)}')\n",
    "print(f'Image shape after transform: {trainset[0][0].shape}')  # (C, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x1000 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Helper function to denormalize images for visualization\n",
    "def imshow(img):\n",
    "    \"\"\"\n",
    "    Display a tensor image by denormalizing and converting to numpy.\n",
    "    \n",
    "    Parameters:\n",
    "    img: PyTorch tensor with shape (C, H, W) in range [-1, 1]\n",
    "    \"\"\"\n",
    "    # Reverse the normalization: x_original = x_normalized * std + mean\n",
    "    # Since we normalized with mean=0.5, std=0.5, we reverse it\n",
    "    img = img * 0.5 + 0.5  # Denormalize from [-1, 1] to [0, 1]\n",
    "    \n",
    "    # Clip values to [0, 1] range (in case of numerical errors)\n",
    "    img = torch.clamp(img, 0, 1)\n",
    "    \n",
    "    # Convert to numpy\n",
    "    # For grayscale images, we need to squeeze the channel dimension\n",
    "    # or transpose to (H, W, C) format for matplotlib\n",
    "    npimg = img.numpy()\n",
    "    if npimg.shape[0] == 1:  # Grayscale\n",
    "        npimg = npimg.squeeze(0)  # Remove channel dimension: (1, H, W) -> (H, W)\n",
    "        plt.imshow(npimg, cmap='gray')\n",
    "    else:  # RGB\n",
    "        npimg = np.transpose(npimg, (1, 2, 0))\n",
    "        plt.imshow(npimg)\n",
    "    \n",
    "    plt.axis('off')  # Hide axes for cleaner visualization\n",
    "\n",
    "# Get a batch of training images\n",
    "# iter() creates an iterator, next() gets the first batch\n",
    "# dataiter.next() returns (images, labels) tuple\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# Create a grid of 16 images (4x4)\n",
    "# images[:16] selects first 16 images from the batch\n",
    "# torchvision.utils.make_grid arranges them in a grid\n",
    "# nrow=4: 4 images per row\n",
    "# padding=2: 2 pixels of padding between images\n",
    "img_grid = torchvision.utils.make_grid(images[:16], nrow=8, padding=1)\n",
    "\n",
    "# Display the grid\n",
    "plt.figure(figsize=(10, 10))\n",
    "imshow(img_grid)\n",
    "plt.title('Downsampled MNIST Images (14x14 pixels)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Designing a Small ConvNet\n",
    "\n",
    "We'll build a simple but effective architecture with relatively small number of parameters for fast CPU training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ConvNet(\n",
      "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=288, out_features=64, bias=True)\n",
      "  (fc2): Linear(in_features=64, out_features=10, bias=True)\n",
      ")\n",
      "\n",
      "Total parameters: 23,946\n",
      "Trainable parameters: 23,946\n"
     ]
    }
   ],
   "source": [
    "class ConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple Convolutional Neural Network for MNIST digit classification.\n",
    "    \n",
    "    Architecture: 2 conv layers + 2 FC layers\n",
    "    Input: 14×14×1 grayscale images\n",
    "    Output: 10 class scores (digits 0-9)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the layers of the ConvNet.\"\"\"\n",
    "        super(ConvNet, self).__init__()\n",
    "        \n",
    "        # First convolutional layer\n",
    "        # nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding)\n",
    "        # in_channels=1: grayscale input (1 channel)\n",
    "        # out_channels=16: learn 16 different filters\n",
    "        # kernel_size=3: 3×3 filters\n",
    "        # padding=1: add 1 pixel of zeros around border (keeps spatial size same)\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Second convolutional layer\n",
    "        # in_channels=16: takes output from conv1\n",
    "        # out_channels=32: learn 32 different filters\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "        # Max pooling layer (applied twice after each conv)\n",
    "        # kernel_size=2: take max over 2×2 regions\n",
    "        # stride=2: move 2 pixels at a time (no overlap)\n",
    "        # This reduces spatial dimensions by factor of 2\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        # First fully connected layer\n",
    "        # After conv1→pool→conv2→pool, spatial size is 14→7→3\n",
    "        # 3×3×32 = 288 features after flattening\n",
    "        # Map 288 → 64 hidden units\n",
    "        self.fc1 = nn.Linear(288, 64)\n",
    "        \n",
    "        # Second fully connected layer (output layer)\n",
    "        # 64 → 10 class scores (one per digit 0-9)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        x: input images, shape (N, 1, 14, 14)\n",
    "           N = batch size, 1 = grayscale channel, 14×14 = spatial dimensions\n",
    "        \n",
    "        Returns:\n",
    "        out: class scores, shape (N, 10)\n",
    "        \"\"\"\n",
    "        # First convolutional block: Conv → ReLU → Pool\n",
    "        # x: (N, 1, 14, 14)\n",
    "        x = self.conv1(x)  # Shape: (N, 16, 14, 14) - 16 feature maps, same spatial size\n",
    "        x = F.relu(x)      # Shape: (N, 16, 14, 14) - ReLU activation (element-wise)\n",
    "        x = self.pool(x)   # Shape: (N, 16, 7, 7) - max pooling reduces spatial size by 2\n",
    "        \n",
    "        # Second convolutional block: Conv → ReLU → Pool\n",
    "        x = self.conv2(x)  # Shape: (N, 32, 7, 7) - 32 feature maps\n",
    "        x = F.relu(x)      # Shape: (N, 32, 7, 7)\n",
    "        x = self.pool(x)   # Shape: (N, 32, 3, 3) - spatial size reduced to 3×3\n",
    "        \n",
    "        # Flatten spatial dimensions\n",
    "        # x.view(x.size(0), -1) reshapes to (N, 32*3*3) = (N, 288)\n",
    "        # x.size(0) is the batch size N\n",
    "        # -1 means \"infer this dimension\" = 32*3*3 = 288\n",
    "        x = x.view(x.size(0), -1)  # Shape: (N, 288)\n",
    "        \n",
    "        # First fully connected layer with ReLU\n",
    "        x = self.fc1(x)    # Shape: (N, 64)\n",
    "        x = F.relu(x)      # Shape: (N, 64)\n",
    "        \n",
    "        # Output layer (no activation - raw scores for cross-entropy loss)\n",
    "        x = self.fc2(x)    # Shape: (N, 10) - one score per class\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Instantiate the model and move to device (CPU or GPU)\n",
    "model = ConvNet().to(device)\n",
    "\n",
    "# Count total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(model)\n",
    "print(f'\\nTotal parameters: {total_params:,}')\n",
    "print(f'Trainable parameters: {trainable_params:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Training batches per epoch: 750\n",
      "Validation batches: 188\n",
      "Epoch [1/5], Batch [100/750], Loss: 2.2165\n",
      "Epoch [1/5], Batch [200/750], Loss: 1.0047\n",
      "Epoch [1/5], Batch [300/750], Loss: 0.4156\n",
      "Epoch [1/5], Batch [400/750], Loss: 0.3164\n",
      "Epoch [1/5], Batch [500/750], Loss: 0.2364\n",
      "Epoch [1/5], Batch [600/750], Loss: 0.1967\n",
      "Epoch [1/5], Batch [700/750], Loss: 0.1499\n",
      "Epoch [1/5] - Train Loss: 0.1276, Val Loss: 0.1320\n",
      "Epoch [2/5], Batch [100/750], Loss: 0.1365\n",
      "Epoch [2/5], Batch [200/750], Loss: 0.1347\n",
      "Epoch [2/5], Batch [300/750], Loss: 0.1054\n",
      "Epoch [2/5], Batch [400/750], Loss: 0.1231\n",
      "Epoch [2/5], Batch [500/750], Loss: 0.0871\n",
      "Epoch [2/5], Batch [600/750], Loss: 0.0969\n",
      "Epoch [2/5], Batch [700/750], Loss: 0.0993\n",
      "Epoch [2/5] - Train Loss: 0.0856, Val Loss: 0.0910\n",
      "Epoch [3/5], Batch [100/750], Loss: 0.0787\n",
      "Epoch [3/5], Batch [200/750], Loss: 0.0778\n",
      "Epoch [3/5], Batch [300/750], Loss: 0.0712\n",
      "Epoch [3/5], Batch [400/750], Loss: 0.0737\n",
      "Epoch [3/5], Batch [500/750], Loss: 0.0777\n",
      "Epoch [3/5], Batch [600/750], Loss: 0.0844\n",
      "Epoch [3/5], Batch [700/750], Loss: 0.0727\n",
      "Epoch [3/5] - Train Loss: 0.0602, Val Loss: 0.0698\n",
      "Epoch [4/5], Batch [100/750], Loss: 0.0603\n",
      "Epoch [4/5], Batch [200/750], Loss: 0.0690\n",
      "Epoch [4/5], Batch [300/750], Loss: 0.0593\n",
      "Epoch [4/5], Batch [400/750], Loss: 0.0682\n",
      "Epoch [4/5], Batch [500/750], Loss: 0.0528\n",
      "Epoch [4/5], Batch [600/750], Loss: 0.0669\n",
      "Epoch [4/5], Batch [700/750], Loss: 0.0653\n",
      "Epoch [4/5] - Train Loss: 0.0537, Val Loss: 0.0656\n",
      "Epoch [5/5], Batch [100/750], Loss: 0.0612\n",
      "Epoch [5/5], Batch [200/750], Loss: 0.0457\n",
      "Epoch [5/5], Batch [300/750], Loss: 0.0569\n",
      "Epoch [5/5], Batch [400/750], Loss: 0.0540\n",
      "Epoch [5/5], Batch [500/750], Loss: 0.0544\n",
      "Epoch [5/5], Batch [600/750], Loss: 0.0555\n",
      "Epoch [5/5], Batch [700/750], Loss: 0.0542\n",
      "Epoch [5/5] - Train Loss: 0.0419, Val Loss: 0.0530\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Define loss function\n",
    "# CrossEntropyLoss expects raw scores (logits), applies softmax internally\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "# SGD with momentum: v_t = momentum * v_{t-1} + gradient\n",
    "#                    w_t = w_{t-1} - lr * v_t\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "\n",
    "# Create validation split from training data\n",
    "# Use 80% for training, 20% for validation\n",
    "train_size = int(0.8 * len(trainset))\n",
    "val_size = len(trainset) - train_size\n",
    "\n",
    "# random_split splits dataset\n",
    "train_subset, val_subset = torch.utils.data.random_split(\n",
    "    trainset, [train_size, val_size]\n",
    ")\n",
    "\n",
    "# Create data loaders for train and validation\n",
    "trainloader_split = torch.utils.data.DataLoader(\n",
    "    train_subset, batch_size=64, shuffle=True, num_workers=2\n",
    ")\n",
    "valloader = torch.utils.data.DataLoader(\n",
    "    val_subset, batch_size=64, shuffle=False, num_workers=2\n",
    ")\n",
    "\n",
    "# Number of epochs (complete passes through training data)\n",
    "num_epochs = 5\n",
    "\n",
    "print('Starting training...')\n",
    "print(f'Training batches per epoch: {len(trainloader_split)}')\n",
    "print(f'Validation batches: {len(valloader)}')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ===== Training phase =====\n",
    "    model.train()  # Set model to training mode (enables dropout, batch norm if present)\n",
    "    \n",
    "    running_loss = 0.0\n",
    "    \n",
    "    # Iterate over training batches\n",
    "    # enumerate() provides batch index i\n",
    "    # trainloader_split yields (images, labels) tuples\n",
    "    for i, (images, labels) in enumerate(trainloader_split):\n",
    "        # Move data to device (CPU or GPU)\n",
    "        images = images.to(device)  # Shape: (64, 3, 16, 16)\n",
    "        labels = labels.to(device)  # Shape: (64,)\n",
    "        \n",
    "        # Zero the parameter gradients from previous iteration\n",
    "        # Gradients accumulate by default, so we must clear them\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: compute predictions\n",
    "        outputs = model(images)  # Shape: (64, 10)\n",
    "        \n",
    "        # Compute loss\n",
    "        # outputs: raw scores (logits), labels: ground truth class indices\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass: compute gradients automatically via backpropagation\n",
    "        # This is where PyTorch shines - no manual gradient calculation!\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights using computed gradients\n",
    "        # optimizer.step() performs: w = w - lr * gradient\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss for this epoch\n",
    "        running_loss += loss.item()\n",
    "        \n",
    "        # Print progress every 100 batches\n",
    "        if i % 100 == 99:\n",
    "            print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "                  f'Batch [{i+1}/{len(trainloader_split)}], '\n",
    "                  f'Loss: {running_loss/100:.4f}')\n",
    "            running_loss = 0.0\n",
    "    \n",
    "    # ===== Validation phase =====\n",
    "    model.eval()  # Set model to evaluation mode (disables dropout, batch norm updates)\n",
    "    \n",
    "    val_loss = 0.0\n",
    "    train_loss_epoch = 0.0\n",
    "    \n",
    "    # Disable gradient computation for validation (saves memory, speeds up)\n",
    "    with torch.no_grad():\n",
    "        # Compute validation loss\n",
    "        for images, labels in valloader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "        \n",
    "        # Compute training loss on full training set\n",
    "        for images, labels in trainloader_split:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            train_loss_epoch += loss.item()\n",
    "    \n",
    "    # Average losses over all batches\n",
    "    train_loss_avg = train_loss_epoch / len(trainloader_split)\n",
    "    val_loss_avg = val_loss / len(valloader)\n",
    "    \n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}] - '\n",
    "          f'Train Loss: {train_loss_avg:.4f}, '\n",
    "          f'Val Loss: {val_loss_avg:.4f}')\n",
    "\n",
    "print('Training complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.66%\n",
      "Correct: 9866/10000\n"
     ]
    }
   ],
   "source": [
    "# Evaluate model performance on the held-out test set\n",
    "\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "# Disable gradient computation\n",
    "with torch.no_grad():\n",
    "    for images, labels in testloader:\n",
    "        # Move to device\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = model(images)  # Shape: (N, 10) - raw scores\n",
    "        \n",
    "        # Convert scores to predicted class\n",
    "        # torch.max returns (values, indices)\n",
    "        # dim=1 means find max along class dimension\n",
    "        # predicted has shape (N,) containing class indices\n",
    "        _, predicted = torch.max(outputs.data, dim=1)\n",
    "        \n",
    "        # Count correct predictions\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = 100 * correct / total\n",
    "print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "print(f'Correct: {correct}/{total}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
