{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rx9gqJ2a3dir"
      },
      "source": [
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mravanba/comp551-notebooks/blob/master/EMforGaussianMixture.ipynb)\n",
        "\n",
        "# Expectation Maximization for Gaussian Mixture Model\n",
        "In K-means each datapoint is assigned to one cluster. An alternative is for each datapoint ($n$) to have a distribution over clusters -- that is $r_{n,k} \\in [0,1]$ and $\\sum_k r_{n,k} = 1$. To do this we can assume each cluster has a Gaussian distribution $\\mathcal{N}(\\mu_k, \\Sigma_k)$. So we assume the data-distrubtion is a **mixture of Gaussians**\n",
        "$$\n",
        "p(x; \\pi, \\{\\mu_k, \\Sigma_k\\}) = \\sum_k \\pi_k \\mathcal{N}(x; \\mu_k, \\Sigma_k)\n",
        "$$\n",
        "where $\\pi_k= p(z=k)$ with $\\sum_k \\pi_k = 1$ defining the weight of each Gaussian in the mixture. These weights should sum to one so that we have a valid pdf. To maximize the logarithm of the marginal-likelihood \n",
        "$\\ell(\\pi, \\{\\mu_k, \\Sigma_k\\}) = \\sum_n \\log \\left ( \\sum_k \\pi_k \\mathcal{N}(x; \\mu_k, \\Sigma_k) \\right)$, we set its partial derivative wrt various parameters to zero. This gives us the value of these parameters in terms of membership probabilities, aka *responsibilities*: $r_{n,k} = p(z=k|x^{(n)})= \\frac{\\pi_k \\mathcal{N}(x; \\mu_k, \\Sigma_k)}{\\sum_c \\pi_c \\mathcal{N}(x; \\mu_c, \\Sigma_c)}$. Since responsibilities are functions of model parmeters, we perform an iterative updating of these two values:\n",
        "1. update responsibilites given the model parameters \n",
        "2. given the responsibilities $r_{n,k}$, update the parameters $\\mu_k, \\Sigma_k$ and $\\pi$. As we said these updates are given by taking the derivative of the log-likelihood:\n",
        "    - New $\\pi_k$ is easy to estimate, it is proportional to the $\\sum_n r_{n,k}$. Since $\\pi_k$ should sum to one we set\n",
        "$$\n",
        "\\pi_k = \\frac{\\sum_n r_{n,k}}{\\sum_{n,c} r_{n,c}}\n",
        "$$\n",
        "    - For $\\mu_k$ and $\\Sigma_k$ we need to estimate mean and covariance of a Gaussian using *weighted* samples, where the (unnormalized) weights for the $k^{th}$ Gaussian in the mixture are $r_{n,k} \\forall n$.\n",
        "\\begin{align}\n",
        "\\mu_k &= \\frac{\\sum_n r_{n,k} x^{(n)}}{\\sum_n r_{n,k}} \\\\\n",
        "\\Sigma_k &= \\frac{ \\sum_n r_{n,k} (x^{(n)} - \\mu_k)(x^{(n)} - \\mu_k)^\\top }{\\sum_n r_{n,k}}\n",
        "\\end{align}\n",
        "The above gives us weighted mean and weighted covariance. We then repeat steps 1 and 2 similar to K-means.\n",
        "\n",
        "Lets implement EM for Gaussian Mixture Model (GMM) below. We re-use our previous impelementation of multivariate Gaussian in GMM."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbX3owch3dis"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "#%matplotlib notebook\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.core.debugger import set_trace\n",
        "import scipy as sp\n",
        "# Set random seed for reproducibility\n",
        "# This ensures same initialization across runs\n",
        "np.random.seed(1234)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z9bVW7Q13djN"
      },
      "outputs": [],
      "source": [
        "# For more comments on multivariate Gaussian class refer to: \n",
        "# https://github.com/mravanba/comp551-notebooks/blob/master/Gaussian.ipynb\n",
        "class Gaussian():\n",
        "    \"\"\"\n",
        "    Multivariate Gaussian distribution.\n",
        "    \n",
        "    Represents a D-dimensional Gaussian with mean mu and covariance Sigma.\n",
        "    Can evaluate the probability density function at given points.\n",
        "    \"\"\"\n",
        "    def __init__(self, mu=0, sigma=0):\n",
        "        \"\"\"\n",
        "        Initialize a Gaussian distribution.\n",
        "        \n",
        "        Parameters:\n",
        "        mu: mean vector, shape (D,) or scalar\n",
        "        sigma: covariance matrix (D, D) or variance (scalar)\n",
        "        \"\"\"\n",
        "        # Ensure mu is at least 1D array, even if scalar is passed\n",
        "        # atleast_1d converts: scalar -> (1,), already 1D -> unchanged\n",
        "        self.mu = np.atleast_1d(mu)\n",
        "        \n",
        "        # Handle sigma based on dimensionality\n",
        "        if np.array(sigma).ndim == 0:\n",
        "            # If sigma is scalar (0D), convert to 2D covariance matrix\n",
        "            # sigma**2 because standard deviation -> variance\n",
        "            # atleast_2d: scalar -> (1,1) matrix\n",
        "            self.Sigma = np.atleast_2d(sigma**2)\n",
        "        else:\n",
        "            # If sigma is already 2D (covariance matrix), use as is\n",
        "            self.Sigma = sigma\n",
        "\n",
        "    def density(self, x):\n",
        "        \"\"\"\n",
        "        Compute probability density at points x.\n",
        "        \n",
        "        Parameters:\n",
        "        x: data points, shape (N, D)\n",
        "        \n",
        "        Returns:\n",
        "        p: density values, shape (N,) - one probability for each point\n",
        "        \"\"\"\n",
        "        N, D = x.shape\n",
        "        \n",
        "        # Center the data: subtract mean from each point\n",
        "        # self.mu has shape (D,)\n",
        "        # self.mu[None,:] adds a dimension -> (1, D) for broadcasting\n",
        "        # x has shape (N, D)\n",
        "        # Broadcasting: (N, D) - (1, D) -> (N, D)\n",
        "        # Result: xm[i,j] = x[i,j] - mu[j]\n",
        "        xm = (x - self.mu[None, :])\n",
        "        \n",
        "        # Normalization constant for multivariate Gaussian\n",
        "        # (2π)^(-D/2) * |Σ|^(-1/2)\n",
        "        # where |Σ| is the determinant of the covariance matrix\n",
        "        normalization = (2*np.pi)**(-D/2) * np.linalg.det(self.Sigma)**(-1/2)\n",
        "        \n",
        "        # Compute Mahalanobis distance: (x-μ)^T Σ^(-1) (x-μ) for each point\n",
        "        # xm @ np.linalg.inv(self.Sigma) has shape (N, D)\n",
        "        # Element-wise multiplication with xm, then sum over D dimensions\n",
        "        # np.sum(..., axis=1) reduces (N, D) -> (N,)\n",
        "        # Result: quadratic[i] = (x[i] - μ)^T Σ^(-1) (x[i] - μ)\n",
        "        quadratic = np.sum((xm @ np.linalg.inv(self.Sigma)) * xm, axis=1)\n",
        "        \n",
        "        # Gaussian formula: normalization * exp(-0.5 * quadratic_form)\n",
        "        # Returns (N,) array of density values\n",
        "        return normalization * np.exp(-.5 * quadratic)\n",
        "\n",
        "\n",
        "class GMM:\n",
        "    \"\"\"\n",
        "    Gaussian Mixture Model using Expectation-Maximization algorithm.\n",
        "    \n",
        "    Fits a mixture of K Gaussian distributions to data by iteratively:\n",
        "    1. E-step: Computing responsibilities (which Gaussian generated each point)\n",
        "    2. M-step: Updating Gaussian parameters given responsibilities\n",
        "    \"\"\"\n",
        "    def __init__(self, K=5, max_iters=200, epsilon=1e-5):\n",
        "        \"\"\"\n",
        "        Initialize GMM parameters.\n",
        "        \n",
        "        Parameters:\n",
        "        K: number of Gaussian components in the mixture\n",
        "        max_iters: maximum number of EM iterations\n",
        "        epsilon: convergence tolerance and regularization constant\n",
        "        \"\"\"\n",
        "        self.K = K                            # Number of Gaussians\n",
        "        self.max_iters = max_iters            # Maximum iterations for EM\n",
        "        self.epsilon = epsilon                # Tolerance for convergence + regularization\n",
        "    \n",
        "    def fit(self, x):\n",
        "        \"\"\"\n",
        "        Fit Gaussian Mixture Model to data using EM algorithm.\n",
        "        \n",
        "        Parameters:\n",
        "        x: data points, shape (N, D) where N=samples, D=features\n",
        "        \n",
        "        Returns:\n",
        "        mu: means of K Gaussians, shape (K, D)\n",
        "        sigma: covariances of K Gaussians, shape (K, D, D)\n",
        "        r: responsibilities (unnormalized), shape (N, K)\n",
        "        \"\"\"\n",
        "        N, D = x.shape\n",
        "        \n",
        "        # ===== INITIALIZATION =====\n",
        "        \n",
        "        # Randomly select K data points as initial cluster centers\n",
        "        # np.random.choice(N, self.K) returns K random integers from [0, N-1]\n",
        "        # These are indices of the chosen data points\n",
        "        init_centers = np.random.choice(N, self.K)\n",
        "        \n",
        "        # Initialize mixing coefficients π_k (weight of each Gaussian)\n",
        "        # Start with uniform weights: each Gaussian has weight 1/K\n",
        "        # Shape: (K,), sums to 1\n",
        "        pi = np.ones(self.K) / self.K\n",
        "        \n",
        "        # Initialize means: use the K randomly selected data points\n",
        "        # x[init_centers] uses advanced indexing to select rows\n",
        "        # Shape: (K, D) - each row is the mean for one Gaussian\n",
        "        mu = x[init_centers]\n",
        "        \n",
        "        # Initialize covariance matrices for all K Gaussians\n",
        "        # Step 1: np.var(x, axis=0) computes variance along each feature\n",
        "        #         Shape: (D,) - one variance per feature\n",
        "        # Step 2: np.diag(...) creates diagonal matrix from these variances\n",
        "        #         Shape: (D, D) - diagonal covariance (assumes features independent)\n",
        "        # Step 3: [None,:,:] adds a dimension -> (1, D, D)\n",
        "        # Step 4: np.tile(..., (self.K, 1, 1)) replicates K times along first axis\n",
        "        #         Shape: (K, D, D) - same diagonal covariance for all K Gaussians\n",
        "        sigma = np.tile(np.diag(np.var(x, axis=0))[None, :, :], (self.K, 1, 1))\n",
        "        \n",
        "        # Initialize responsibilities matrix\n",
        "        # r[n,k] will store π_k * p(x_n | μ_k, Σ_k)\n",
        "        # Shape: (N, K) - each row is one data point, each column is one Gaussian\n",
        "        r = np.zeros((N, self.K))\n",
        "        \n",
        "        # Initialize log-likelihood to negative infinity\n",
        "        # Will be updated in each iteration to check convergence\n",
        "        ll = -np.inf\n",
        "        \n",
        "        # ===== EM ALGORITHM =====\n",
        "        \n",
        "        for t in range(self.max_iters):\n",
        "            # ----- E-STEP: Update responsibilities -----\n",
        "            \n",
        "            # For each Gaussian k, compute π_k * p(x | μ_k, Σ_k) for all points\n",
        "            for i in range(self.K):\n",
        "                # Gaussian(mu[i], sigma[i]).density(x) computes p(x | μ_i, Σ_i)\n",
        "                # Shape: (N,) - density for all N points under Gaussian i\n",
        "                # Multiply by pi[i] (mixing coefficient) to get π_i * p(x | μ_i, Σ_i)\n",
        "                # r[:,i] assigns to column i (all rows, column i)\n",
        "                r[:, i] = pi[i] * Gaussian(mu[i], sigma[i]).density(x)\n",
        "            \n",
        "            # Normalize responsibilities to get posterior p(z=k | x)\n",
        "            # np.sum(r, axis=1) sums across Gaussians (axis 1), shape: (N,)\n",
        "            # keepdims=True preserves dimension: (N,) -> (N, 1)\n",
        "            # This allows broadcasting: (N, K) / (N, 1) -> (N, K)\n",
        "            # After normalization: each row sums to 1\n",
        "            # r_norm[n,k] = π_k * p(x_n|k) / Σ_c π_c * p(x_n|c) = p(z=k | x_n)\n",
        "            r_norm = r / np.sum(r, axis=1, keepdims=True)\n",
        "\n",
        "            # ----- M-STEP: Update parameters given responsibilities -----\n",
        "            \n",
        "            for i in range(self.K):\n",
        "                # Update mean μ_k as weighted average of data\n",
        "                # np.average(x, axis=0, weights=r_norm[:,i])\n",
        "                # - x has shape (N, D)\n",
        "                # - weights=r_norm[:,i] has shape (N,) - responsibility for Gaussian i\n",
        "                # - axis=0 means average over samples (rows)\n",
        "                # Result: μ_i = Σ_n r_{n,i} * x_n / Σ_n r_{n,i}\n",
        "                # Shape: (D,) - one mean value per feature\n",
        "                mu[i, :] = np.average(x, axis=0, weights=r_norm[:, i])\n",
        "                \n",
        "                # Update covariance Σ_k as weighted covariance matrix\n",
        "                # np.cov(x, aweights=r_norm[:,i], rowvar=False)\n",
        "                # - x has shape (N, D)\n",
        "                # - aweights=r_norm[:,i] are the analytic weights (responsibilities)\n",
        "                # - rowvar=False: each column is a variable (not each row)\n",
        "                # Result: Σ_i = Σ_n r_{n,i} * (x_n - μ_i)(x_n - μ_i)^T / Σ_n r_{n,i}\n",
        "                # Shape: (D, D)\n",
        "                # Add ε*I for regularization to prevent singular matrices\n",
        "                # self.epsilon * np.eye(D) creates diagonal matrix with ε on diagonal\n",
        "                # [None,:,:] adds dimension: (D,D) -> (1,D,D) for assignment to sigma[i]\n",
        "                sigma[i, :, :] = np.cov(x, aweights=r_norm[:, i], rowvar=False) + \\\n",
        "                                  self.epsilon * np.eye(D)[None, :, :]\n",
        "            \n",
        "            # Update mixing coefficients π_k\n",
        "            # π_k should be proportional to total responsibility assigned to Gaussian k\n",
        "            # np.sum(r_norm, axis=0) sums over data points (axis 0)\n",
        "            # Shape: (K,) - one value per Gaussian\n",
        "            # Result: π_k ∝ Σ_n r_{n,k}\n",
        "            pi = np.sum(r_norm, axis=0)\n",
        "            # Normalize so Σ_k π_k = 1 (valid probability distribution)\n",
        "            pi /= np.sum(pi)\n",
        "            \n",
        "            # ----- CHECK CONVERGENCE -----\n",
        "            \n",
        "            # Calculate log-likelihood: log p(x) = log Σ_k π_k p(x|k)\n",
        "            # np.sum(r, axis=1) computes Σ_k π_k p(x_n|k) for each point, shape (N,)\n",
        "            # np.log(...) takes logarithm\n",
        "            # np.mean(...) averages over all data points\n",
        "            ll_new = np.mean(np.log(np.sum(r, axis=1)))\n",
        "            \n",
        "            # Check if change in log-likelihood is below tolerance\n",
        "            if np.abs(ll_new - ll) < self.epsilon:\n",
        "                print(f'converged after {t} iterations, average log-likelihood {ll_new}')\n",
        "                break\n",
        "            \n",
        "            # Update log-likelihood for next iteration\n",
        "            ll = ll_new\n",
        "        \n",
        "        return mu, sigma, r"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3J0SWX73djO"
      },
      "source": [
        "In the implementation above note that we add a diagonal matrix with small constant values ($\\epsilon$) on the diagonal to the empirical covariance matrix. This is to avoid degeneracy. If the model puts one of the Gaussians centered on a data-points and make the covariance very small, it can arbitrarily increase the likelihood of the data (see Bishop p.434). This addition of epsilon to the diagonal prevents this degeneracy.\n",
        "\n",
        "Now let's apply this to the Iris dataset "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h1_s2m6Z3djO"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "\n",
        "# Load the famous Iris dataset\n",
        "dataset = datasets.load_iris()\n",
        "\n",
        "# Extract features and labels\n",
        "# dataset['data'] has shape (150, 4) - 150 samples, 4 features\n",
        "# [:,:2] uses slicing to select only first 2 features (sepal length and width)\n",
        "# This allows us to visualize in 2D\n",
        "# x has shape (150, 2)\n",
        "# y has shape (150,) - contains class labels 0, 1, or 2\n",
        "x, y = dataset['data'][:, :2], dataset['target']\n",
        "\n",
        "# Fit Gaussian Mixture Model with K=3 components\n",
        "gmm = GMM(K=3)\n",
        "mu, sigma, resp = gmm.fit(x)\n",
        "\n",
        "# Normalize responsibilities to get posterior probabilities\n",
        "# resp has shape (N, K) where each entry is π_k * p(x_n|k)\n",
        "# np.sum(resp, axis=1, keepdims=True) has shape (N, 1)\n",
        "# After division: each row sums to 1, representing p(z=k|x_n) for each k\n",
        "resp /= np.sum(resp, axis=1, keepdims=True)\n",
        "\n",
        "# ===== CREATE VISUALIZATION WITH 3 SUBPLOTS =====\n",
        "\n",
        "fig, axes = plt.subplots(ncols=3, nrows=1, constrained_layout=True, figsize=(15, 5))\n",
        "\n",
        "# ----- LEFT PLOT: GMM clustering with contours -----\n",
        "\n",
        "# Plot data points colored by responsibilities\n",
        "# resp has shape (N, K), used as RGB colors for scatter plot\n",
        "# Each point's color is a mix of the K Gaussians it belongs to\n",
        "# s=2 sets small marker size\n",
        "axes[0].scatter(x[:, 0], x[:, 1], c=resp, s=2)\n",
        "\n",
        "# Plot cluster centers (means) with 'x' markers\n",
        "# mu has shape (K, D) where D=2\n",
        "# mu[:,0] selects first feature (x-coordinate), shape (K,)\n",
        "# mu[:,1] selects second feature (y-coordinate), shape (K,)\n",
        "axes[0].scatter(mu[:, 0], mu[:, 1], marker='x')\n",
        "\n",
        "# Create a dense grid for visualizing Gaussian density contours\n",
        "# np.linspace creates evenly spaced values across the data range\n",
        "# x0v and x1v are 1D arrays of length 200\n",
        "x0v = np.linspace(np.min(x[:, 0]), np.max(x[:, 0]), 200)\n",
        "x1v = np.linspace(np.min(x[:, 1]), np.max(x[:, 1]), 200)\n",
        "\n",
        "# Create 2D meshgrid\n",
        "# x0 and x1 are both (200, 200) arrays\n",
        "# x0[i,j] contains the first feature value at grid point (i,j)\n",
        "# x1[i,j] contains the second feature value at grid point (i,j)\n",
        "x0, x1 = np.meshgrid(x0v, x1v)\n",
        "\n",
        "# Flatten and stack to create array of all grid points\n",
        "# x0.ravel() flattens (200, 200) -> (40000,)\n",
        "# x1.ravel() flattens (200, 200) -> (40000,)\n",
        "# np.vstack stacks vertically to create (2, 40000)\n",
        "# .T transposes to get (40000, 2)\n",
        "# Each row is a 2D point [x0, x1] in the feature space\n",
        "x_all = np.vstack((x0.ravel(), x1.ravel())).T\n",
        "\n",
        "# Get density values for each Gaussian at all grid points\n",
        "# Gaussian(mu[0], sigma[0]).density(x_all) evaluates first Gaussian\n",
        "# Returns shape (40000,) - density at each grid point\n",
        "p0 = Gaussian(mu[0], sigma[0]).density(x_all)\n",
        "p1 = Gaussian(mu[1], sigma[1]).density(x_all)\n",
        "p2 = Gaussian(mu[2], sigma[2]).density(x_all)\n",
        "\n",
        "# Stack densities to create mixture\n",
        "# np.vstack([p0, p1, p2]) has shape (3, 40000)\n",
        "# .T transposes to (40000, 3)\n",
        "# Each row represents densities [p0, p1, p2] at one grid point\n",
        "p = np.vstack([p0, p1, p2]).T\n",
        "\n",
        "# Normalize to get responsibilities at each grid point\n",
        "# np.sum(p, axis=1, keepdims=True) has shape (40000, 1)\n",
        "# After division: each row sums to 1\n",
        "# p[i,k] = p_k(grid_point_i) / Σ_j p_j(grid_point_i)\n",
        "p /= np.sum(p, axis=1, keepdims=True)\n",
        "\n",
        "# Draw contour lines for each Gaussian's density\n",
        "# tricontour works with unstructured grids (x_all is flattened)\n",
        "# x_all[:,0] is first feature (x-coordinates), shape (40000,)\n",
        "# x_all[:,1] is second feature (y-coordinates), shape (40000,)\n",
        "# p0, p1, p2 are density values, shape (40000,)\n",
        "# This draws level curves showing regions of equal probability density\n",
        "axes[0].tricontour(x_all[:, 0], x_all[:, 1], p0)\n",
        "axes[0].tricontour(x_all[:, 0], x_all[:, 1], p1)\n",
        "axes[0].tricontour(x_all[:, 0], x_all[:, 1], p2)\n",
        "axes[0].set_title('Mixture of Gaussians found using EM')\n",
        "\n",
        "# ----- MIDDLE PLOT: Cluster responsibilities as continuous colors -----\n",
        "\n",
        "# Plot training data colored by true labels\n",
        "# y contains integers 0, 1, 2 representing true classes\n",
        "axes[1].scatter(x[:, 0], x[:, 1], c=y, s=2)\n",
        "\n",
        "# Overlay grid points colored by responsibilities\n",
        "# p is (40000, 3) - RGB-like colors showing cluster membership\n",
        "# marker='.' creates tiny dots\n",
        "# alpha=.01 makes points very transparent to show smooth gradient\n",
        "# This creates a background showing how the model divides the space\n",
        "axes[1].scatter(x_all[:, 0], x_all[:, 1], c=p, marker='.', alpha=.01)\n",
        "axes[1].set_title('Cluster Responsibilities')\n",
        "\n",
        "# ----- RIGHT PLOT: True class labels for comparison -----\n",
        "\n",
        "# Plot data colored by true labels (no model predictions)\n",
        "# This shows the ground truth for comparison with model output\n",
        "axes[2].scatter(x[:, 0], x[:, 1], c=y, s=2)\n",
        "axes[2].set_title('class labels')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUNPjWv03djO"
      },
      "source": [
        "A mixture of Gaussians where the membership of each point is unobserved is an example of a **latent variable model**.\n",
        "A latent variable model $p(x,z; \\theta)$ assumes unobserved or latent variables $z$ that help explain the observations $x$. In this setup we still want to maximize the **marginal likelihood** of data ($z$ is marginalized out):\n",
        "$$\n",
        "\\max_\\theta \\sum_n \\log p(x^{(n)}; \\theta) = \\max_\\theta \\sum_n \\log \\sum_z p(x^{(n)}, z; \\theta)\n",
        "$$\n",
        "EM can be used for learning with *latent variable models* or when we have *missing data*. The general approach is similar to what we saw here: \n",
        "- computing the posterior $p(z | x^{(n)}; \\theta) \\forall n$ (Expectation or **E step**)\n",
        "- maximizing the expected log-likelihood using this probabilistic *completion* of the data (Maximization or **M step**)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "EMforGaussianMixture.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
