{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mravanba/comp551-notebooks/blob/master/RNN.ipynb)\n",
    "\n",
    "# Recurrent Neural Networks (RNNs) for Text\n",
    "\n",
    "Our goal is to implement a Recurrent Neural Network (RNN) for sentiment analysis using PyTorch. We'll build a simple text classifier that predicts whether a movie review is positive or negative. To make training faster on CPU, we'll use a limited vocabulary and shorter sequences.\n",
    "\n",
    "RNNs are designed to process sequential data by maintaining a hidden state that captures information from previous time steps. Unlike feedforward networks that process each input independently, RNNs:\n",
    "- **Process sequences step-by-step**: Handle variable-length inputs (e.g., sentences of different lengths)\n",
    "- **Maintain hidden state**: Carry information forward through the sequence\n",
    "- **Share parameters across time**: Use the same weights at each time step, making them efficient for sequential data\n",
    "\n",
    "For text data, RNNs can learn patterns in word sequences to understand context and meaning, making them suitable for tasks like sentiment analysis, language modeling, and machine translation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "# Suppress warnings for cleaner output\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# PyTorch imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# For text processing\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "# This ensures results are consistent across runs\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Simple Sentiment Dataset\n",
    "\n",
    "For this tutorial, we'll create a simple movie review dataset for demonstration. Our toy dataset will have:\n",
    "- **Positive reviews**: Reviews with positive sentiment\n",
    "- **Negative reviews**: Reviews with negative sentiment\n",
    "- **Binary classification**: Predict 0 (negative) or 1 (positive)\n",
    "\n",
    "We'll process the text by:\n",
    "1. Converting to lowercase\n",
    "2. Building a vocabulary from the training data\n",
    "3. Converting words to integer indices\n",
    "4. Padding sequences to a fixed length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 40\n",
      "Test samples: 10\n",
      "\n",
      "Example positive review: \"this movie is great and fantastic\"\n",
      "Example negative review: \"this movie is terrible and boring\"\n"
     ]
    }
   ],
   "source": [
    "# Simple movie review dataset\n",
    "# In practice, you would load this from a file or use a real dataset\n",
    "train_texts = [\n",
    "    # Positive reviews\n",
    "    \"this movie is great and fantastic\",\n",
    "    \"i loved this film it was amazing\",\n",
    "    \"excellent movie with great acting\",\n",
    "    \"wonderful story and beautiful cinematography\",\n",
    "    \"best movie i have seen this year\",\n",
    "    \"brilliant film highly recommend it\",\n",
    "    \"amazing performance by the actors\",\n",
    "    \"loved every minute of this movie\",\n",
    "    \"fantastic plot and great direction\",\n",
    "    \"this is an excellent film\",\n",
    "    \"superb acting and wonderful story\",\n",
    "    \"incredible movie worth watching\",\n",
    "    \"great entertainment and amazing visuals\",\n",
    "    \"perfect movie for the weekend\",\n",
    "    \"outstanding performance and direction\",\n",
    "    \"beautiful film with touching story\",\n",
    "    \"impressive acting and great script\",\n",
    "    \"enjoyed this movie very much\",\n",
    "    \"brilliant direction and excellent cast\",\n",
    "    \"wonderful experience watching this film\",\n",
    "    \n",
    "    # Negative reviews\n",
    "    \"this movie is terrible and boring\",\n",
    "    \"i hated this film it was awful\",\n",
    "    \"horrible movie with bad acting\",\n",
    "    \"terrible story and poor direction\",\n",
    "    \"worst movie i have seen this year\",\n",
    "    \"disappointing film waste of time\",\n",
    "    \"awful performance by the actors\",\n",
    "    \"disliked every minute of this movie\",\n",
    "    \"poor plot and bad direction\",\n",
    "    \"this is a terrible film\",\n",
    "    \"bad acting and horrible story\",\n",
    "    \"dreadful movie not worth watching\",\n",
    "    \"boring entertainment and poor visuals\",\n",
    "    \"waste of time and money\",\n",
    "    \"disappointing performance and direction\",\n",
    "    \"awful film with weak story\",\n",
    "    \"poor acting and bad script\",\n",
    "    \"did not enjoy this movie\",\n",
    "    \"terrible direction and weak cast\",\n",
    "    \"unpleasant experience watching this film\",\n",
    "]\n",
    "\n",
    "# Labels: 1 for positive (first 20), 0 for negative (last 20)\n",
    "train_labels = [1] * 20 + [0] * 20\n",
    "\n",
    "# Test set (similar structure)\n",
    "test_texts = [\n",
    "    \"wonderful movie highly recommended\",\n",
    "    \"great film with amazing story\",\n",
    "    \"excellent acting and direction\",\n",
    "    \"loved this amazing film\",\n",
    "    \"fantastic movie worth watching\",\n",
    "    \"terrible movie very disappointing\",\n",
    "    \"awful film waste of time\",\n",
    "    \"horrible acting and poor story\",\n",
    "    \"hated this terrible movie\",\n",
    "    \"bad film not recommended\",\n",
    "]\n",
    "\n",
    "test_labels = [1, 1, 1, 1, 1, 0, 0, 0, 0, 0]\n",
    "\n",
    "print(f'Training samples: {len(train_texts)}')\n",
    "print(f'Test samples: {len(test_texts)}')\n",
    "print(f'\\nExample positive review: \"{train_texts[0]}\"')\n",
    "print(f'Example negative review: \"{train_texts[20]}\"')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing and Vocabulary Building\n",
    "\n",
    "To feed text into a neural network, we need to convert words to numbers:\n",
    "\n",
    "1. **Tokenization**: Split text into individual words\n",
    "2. **Vocabulary**: Create a mapping from words to unique integer indices\n",
    "3. **Numericalization**: Convert each word in a sentence to its index\n",
    "4. **Padding**: Make all sequences the same length by adding padding tokens\n",
    "\n",
    "We also add special tokens:\n",
    "- `<PAD>`: Padding token (index 0) for shorter sequences\n",
    "- `<UNK>`: Unknown token (index 1) for words not in vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 76\n",
      "\n",
      "First 20 words in vocabulary:\n",
      "0: <PAD>\n",
      "1: <UNK>\n",
      "2: and\n",
      "3: this\n",
      "4: movie\n",
      "5: film\n",
      "6: direction\n",
      "7: acting\n",
      "8: story\n",
      "9: great\n",
      "10: is\n",
      "11: i\n",
      "12: with\n",
      "13: performance\n",
      "14: of\n",
      "15: watching\n",
      "16: terrible\n",
      "17: bad\n",
      "18: poor\n",
      "19: it\n"
     ]
    }
   ],
   "source": [
    "# Simple tokenization function\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Convert text to lowercase and split into words.\n",
    "    \n",
    "    Parameters:\n",
    "    text: input string\n",
    "    \n",
    "    Returns:\n",
    "    list of tokens (words)\n",
    "    \"\"\"\n",
    "    # Convert to lowercase and remove punctuation\n",
    "    text = text.lower()\n",
    "    # Split on whitespace\n",
    "    tokens = text.split()\n",
    "    return tokens\n",
    "\n",
    "# Build vocabulary from training data\n",
    "def build_vocab(texts, max_vocab_size=1000):\n",
    "    \"\"\"\n",
    "    Build a vocabulary from a list of texts.\n",
    "    \n",
    "    Parameters:\n",
    "    texts: list of text strings\n",
    "    max_vocab_size: maximum vocabulary size (most frequent words)\n",
    "    \n",
    "    Returns:\n",
    "    word2idx: dictionary mapping words to indices\n",
    "    idx2word: dictionary mapping indices to words\n",
    "    \"\"\"\n",
    "    # Count word frequencies\n",
    "    word_counts = Counter()\n",
    "    for text in texts:\n",
    "        tokens = tokenize(text)\n",
    "        word_counts.update(tokens)\n",
    "    \n",
    "    # Get most common words\n",
    "    # Reserve index 0 for padding and index 1 for unknown words\n",
    "    most_common = word_counts.most_common(max_vocab_size - 2)\n",
    "    \n",
    "    # Create word to index mapping\n",
    "    # Start from index 2 (0 is <PAD>, 1 is <UNK>)\n",
    "    word2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "    for idx, (word, count) in enumerate(most_common, start=2):\n",
    "        word2idx[word] = idx\n",
    "    \n",
    "    # Create index to word mapping (inverse)\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    \n",
    "    return word2idx, idx2word\n",
    "\n",
    "# Build vocabulary from training texts\n",
    "word2idx, idx2word = build_vocab(train_texts, max_vocab_size=1000)\n",
    "vocab_size = len(word2idx)\n",
    "\n",
    "print(f'Vocabulary size: {vocab_size}')\n",
    "print(f'\\nFirst 20 words in vocabulary:')\n",
    "for i in range(min(20, vocab_size)):\n",
    "    print(f'{i}: {idx2word[i]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (40, 10)\n",
      "Test data shape: (10, 10)\n",
      "\n",
      "Example sequence (first training sample):\n",
      "Text: \"this movie is great and fantastic\"\n",
      "Indices: [ 3  4 10  9  2 25  0  0  0  0]\n",
      "Label: 1 (positive)\n"
     ]
    }
   ],
   "source": [
    "# Convert text to sequence of indices\n",
    "def text_to_sequence(text, word2idx):\n",
    "    \"\"\"\n",
    "    Convert a text string to a sequence of word indices.\n",
    "    \n",
    "    Parameters:\n",
    "    text: input text string\n",
    "    word2idx: word to index mapping dictionary\n",
    "    \n",
    "    Returns:\n",
    "    list of word indices\n",
    "    \"\"\"\n",
    "    tokens = tokenize(text)\n",
    "    # Convert each token to its index\n",
    "    # Use index 1 (<UNK>) for words not in vocabulary\n",
    "    sequence = [word2idx.get(token, 1) for token in tokens]\n",
    "    return sequence\n",
    "\n",
    "# Pad sequences to fixed length\n",
    "def pad_sequences(sequences, max_length, pad_value=0):\n",
    "    \"\"\"\n",
    "    Pad sequences to a fixed length.\n",
    "    \n",
    "    Parameters:\n",
    "    sequences: list of sequences (lists of integers)\n",
    "    max_length: target length for all sequences\n",
    "    pad_value: value to use for padding (default: 0 for <PAD>)\n",
    "    \n",
    "    Returns:\n",
    "    numpy array of shape (num_sequences, max_length)\n",
    "    \"\"\"\n",
    "    padded = np.zeros((len(sequences), max_length), dtype=np.int64)\n",
    "    \n",
    "    for i, seq in enumerate(sequences):\n",
    "        # Truncate if longer than max_length\n",
    "        seq = seq[:max_length]\n",
    "        # Copy sequence to padded array\n",
    "        padded[i, :len(seq)] = seq\n",
    "    \n",
    "    return padded\n",
    "\n",
    "# Set maximum sequence length (number of words per review)\n",
    "# Shorter sequences are padded, longer ones are truncated\n",
    "MAX_LENGTH = 10\n",
    "\n",
    "# Convert training texts to sequences\n",
    "train_sequences = [text_to_sequence(text, word2idx) for text in train_texts]\n",
    "test_sequences = [text_to_sequence(text, word2idx) for text in test_texts]\n",
    "\n",
    "# Pad sequences\n",
    "X_train = pad_sequences(train_sequences, MAX_LENGTH)\n",
    "X_test = pad_sequences(test_sequences, MAX_LENGTH)\n",
    "\n",
    "# Convert labels to numpy arrays\n",
    "y_train = np.array(train_labels)\n",
    "y_test = np.array(test_labels)\n",
    "\n",
    "print(f'Training data shape: {X_train.shape}')\n",
    "print(f'Test data shape: {X_test.shape}')\n",
    "print(f'\\nExample sequence (first training sample):')\n",
    "print(f'Text: \"{train_texts[0]}\"')\n",
    "print(f'Indices: {X_train[0]}')\n",
    "print(f'Label: {y_train[0]} (positive)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating PyTorch Dataset and DataLoader\n",
    "\n",
    "We'll create a custom PyTorch Dataset class to handle our text data. The DataLoader will batch our data and shuffle it during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training batches: 5\n",
      "Number of test batches: 2\n",
      "\n",
      "Batch shapes:\n",
      "Sequences: torch.Size([8, 10])  # (batch_size, sequence_length)\n",
      "Labels: torch.Size([8])  # (batch_size,)\n"
     ]
    }
   ],
   "source": [
    "class SentimentDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset for sentiment analysis.\n",
    "    \n",
    "    This class wraps our preprocessed sequences and labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, sequences, labels):\n",
    "        \"\"\"\n",
    "        Initialize the dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        sequences: numpy array of shape (num_samples, max_length)\n",
    "        labels: numpy array of shape (num_samples,)\n",
    "        \"\"\"\n",
    "        self.sequences = torch.LongTensor(sequences)\n",
    "        self.labels = torch.LongTensor(labels)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the number of samples in the dataset.\"\"\"\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Get a single sample from the dataset.\n",
    "        \n",
    "        Parameters:\n",
    "        idx: index of the sample to retrieve\n",
    "        \n",
    "        Returns:\n",
    "        tuple of (sequence, label)\n",
    "        \"\"\"\n",
    "        return self.sequences[idx], self.labels[idx]\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = SentimentDataset(X_train, y_train)\n",
    "test_dataset = SentimentDataset(X_test, y_test)\n",
    "\n",
    "# Create data loaders\n",
    "# batch_size=8: small batch size for CPU-friendly training\n",
    "# shuffle=True: randomize training order each epoch\n",
    "trainloader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "testloader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n",
    "\n",
    "print(f'Number of training batches: {len(trainloader)}')\n",
    "print(f'Number of test batches: {len(testloader)}')\n",
    "\n",
    "# Examine a batch\n",
    "sample_batch = next(iter(trainloader))\n",
    "sequences_batch, labels_batch = sample_batch\n",
    "print(f'\\nBatch shapes:')\n",
    "print(f'Sequences: {sequences_batch.shape}  # (batch_size, sequence_length)')\n",
    "print(f'Labels: {labels_batch.shape}  # (batch_size,)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementing a Vanilla RNN from Scratch\n",
    "\n",
    "We'll build a simple RNN-based sentiment classifier by implementing the RNN computation manually. This helps us understand exactly how RNNs work internally.\n",
    "\n",
    "**Architecture:**\n",
    "\n",
    "1. **Embedding Layer**: Converts word indices to dense vectors (word embeddings)\n",
    "   - Maps each word index to a learnable vector of size `embedding_dim`\n",
    "   - Similar words will have similar embeddings after training\n",
    "\n",
    "2. **RNN Layer** (implemented from scratch): Processes the sequence of word embeddings\n",
    "   - At each time step t, RNN computes: **h_t = tanh(W_ih @ x_t + W_hh @ h_{t-1} + b)**\n",
    "   - `h_t`: hidden state at time t (captures information from all previous words)\n",
    "   - `x_t`: input embedding at time t (from the embedding layer)\n",
    "   - `W_ih`: weight matrix for input-to-hidden transformation\n",
    "   - `W_hh`: weight matrix for hidden-to-hidden transformation (recurrent connection)\n",
    "   - `b`: bias term\n",
    "   - The hidden state is updated sequentially as we process each word in the sequence\n",
    "\n",
    "3. **Fully Connected Layer**: Maps final hidden state to class scores\n",
    "   - Takes the last hidden state (after processing all words)\n",
    "   - Outputs 2 scores (one for each class: negative/positive)\n",
    "\n",
    "We'll implement the RNN computation manually using basic PyTorch operations to see how the sequential processing works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VanillaRNN(\n",
      "  (embedding): Embedding(76, 32, padding_idx=0)\n",
      "  (input_to_hidden): Linear(in_features=32, out_features=64, bias=True)\n",
      "  (hidden_to_hidden): Linear(in_features=64, out_features=64, bias=False)\n",
      "  (fc): Linear(in_features=64, out_features=2, bias=True)\n",
      ")\n",
      "\n",
      "Total parameters: 8,770\n",
      "Trainable parameters: 8,770\n",
      "\n",
      "RNN weight matrices:\n",
      "W_ih (input-to-hidden): 32 × 64 = 2,048 parameters\n",
      "W_hh (hidden-to-hidden): 64 × 64 = 4,096 parameters\n",
      "Embedding: 76 × 32 = 2,432 parameters\n",
      "Output layer: 64 × 2 = 128 parameters\n"
     ]
    }
   ],
   "source": [
    "class VanillaRNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Simple RNN for sentiment classification, implemented from scratch.\n",
    "    \n",
    "    Architecture: Embedding → RNN (manual implementation) → Fully Connected\n",
    "    Input: Sequences of word indices (batch_size, seq_length)\n",
    "    Output: Class scores (batch_size, num_classes)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):\n",
    "        \"\"\"\n",
    "        Initialize the RNN layers.\n",
    "        \n",
    "        Parameters:\n",
    "        vocab_size: size of the vocabulary (number of unique words)\n",
    "        embedding_dim: dimension of word embeddings (e.g., 32, 50)\n",
    "        hidden_dim: dimension of RNN hidden state\n",
    "        output_dim: number of output classes (2 for binary sentiment)\n",
    "        \"\"\"\n",
    "        super(VanillaRNN, self).__init__()\n",
    "        \n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Embedding layer: converts word indices to dense vectors\n",
    "        # vocab_size: number of words in vocabulary\n",
    "        # embedding_dim: size of each embedding vector\n",
    "        # padding_idx=0: the <PAD> token (index 0) will have zero embedding\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        \n",
    "        # RNN parameters (implemented manually)\n",
    "        # Instead of using nn.RNN, we define the weight matrices explicitly\n",
    "        \n",
    "        # W_ih: input-to-hidden weight matrix\n",
    "        # Maps input x_t (embedding_dim) to hidden space (hidden_dim)\n",
    "        # This linear layer computes W_ih @ x_t + b_ih\n",
    "        self.input_to_hidden = nn.Linear(embedding_dim, hidden_dim)\n",
    "        \n",
    "        # W_hh: hidden-to-hidden weight matrix (recurrent connection)\n",
    "        # Maps previous hidden state h_{t-1} (hidden_dim) to hidden space (hidden_dim)\n",
    "        # This linear layer computes W_hh @ h_{t-1} + b_hh\n",
    "        # Note: We set bias=False here because we already have bias in input_to_hidden\n",
    "        self.hidden_to_hidden = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
    "        \n",
    "        # Fully connected layer: maps final hidden state to class scores\n",
    "        # hidden_dim → output_dim (2 classes for binary sentiment)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass through the network.\n",
    "        \n",
    "        Parameters:\n",
    "        x: input sequences, shape (batch_size, seq_length)\n",
    "           Each element is a word index\n",
    "        \n",
    "        Returns:\n",
    "        out: class scores, shape (batch_size, output_dim)\n",
    "        \"\"\"\n",
    "        # Embedding layer\n",
    "        # Input: (batch_size, seq_length)\n",
    "        # Output: (batch_size, seq_length, embedding_dim)\n",
    "        embedded = self.embedding(x)\n",
    "        \n",
    "        batch_size = embedded.size(0)\n",
    "        seq_length = embedded.size(1)\n",
    "        \n",
    "        # Initialize hidden state with zeros\n",
    "        # Shape: (batch_size, hidden_dim)\n",
    "        # This is h_0, the initial hidden state before processing any input\n",
    "        hidden = torch.zeros(batch_size, self.hidden_dim).to(x.device)\n",
    "        \n",
    "        # Process sequence step by step (this is the core RNN computation)\n",
    "        # We manually iterate through each time step to show how RNNs work\n",
    "        for t in range(seq_length):\n",
    "            # Get input at current time step\n",
    "            # x_t has shape: (batch_size, embedding_dim)\n",
    "            x_t = embedded[:, t, :]\n",
    "            \n",
    "            # RNN computation: h_t = tanh(W_ih @ x_t + W_hh @ h_{t-1} + b)\n",
    "            # This is the fundamental RNN equation!\n",
    "            \n",
    "            # Step 1: Transform input through input-to-hidden weights\n",
    "            # input_to_hidden computes: W_ih @ x_t + b_ih\n",
    "            # Shape: (batch_size, hidden_dim)\n",
    "            input_contribution = self.input_to_hidden(x_t)\n",
    "            \n",
    "            # Step 2: Transform previous hidden state through hidden-to-hidden weights\n",
    "            # hidden_to_hidden computes: W_hh @ h_{t-1}\n",
    "            # Shape: (batch_size, hidden_dim)\n",
    "            hidden_contribution = self.hidden_to_hidden(hidden)\n",
    "            \n",
    "            # Step 3: Combine both contributions and apply tanh activation\n",
    "            # This creates the new hidden state that incorporates both:\n",
    "            # - Information from current input (x_t)\n",
    "            # - Information from previous hidden state (h_{t-1})\n",
    "            hidden = torch.tanh(input_contribution + hidden_contribution)\n",
    "            \n",
    "            # The hidden state now contains information from all words seen so far\n",
    "            # (from word 0 to word t)\n",
    "        \n",
    "        # After processing all time steps, 'hidden' contains the final hidden state\n",
    "        # This final hidden state has \"seen\" the entire sequence\n",
    "        # Shape: (batch_size, hidden_dim)\n",
    "        \n",
    "        # Fully connected layer: map final hidden state to class scores\n",
    "        # Input: (batch_size, hidden_dim)\n",
    "        # Output: (batch_size, output_dim)\n",
    "        out = self.fc(hidden)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM = 32   # Size of word embeddings (smaller for CPU efficiency)\n",
    "HIDDEN_DIM = 64      # Size of RNN hidden state\n",
    "OUTPUT_DIM = 2       # Binary classification (negative/positive)\n",
    "\n",
    "# Instantiate the model and move to device\n",
    "model = VanillaRNN(vocab_size, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(model)\n",
    "print(f'\\nTotal parameters: {total_params:,}')\n",
    "print(f'Trainable parameters: {trainable_params:,}')\n",
    "print(f'\\nRNN weight matrices:')\n",
    "print(f'W_ih (input-to-hidden): {EMBEDDING_DIM} × {HIDDEN_DIM} = {EMBEDDING_DIM * HIDDEN_DIM:,} parameters')\n",
    "print(f'W_hh (hidden-to-hidden): {HIDDEN_DIM} × {HIDDEN_DIM} = {HIDDEN_DIM * HIDDEN_DIM:,} parameters')\n",
    "print(f'Embedding: {vocab_size} × {EMBEDDING_DIM} = {vocab_size * EMBEDDING_DIM:,} parameters')\n",
    "print(f'Output layer: {HIDDEN_DIM} × {OUTPUT_DIM} = {HIDDEN_DIM * OUTPUT_DIM:,} parameters')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the RNN\n",
    "\n",
    "The training loop:\n",
    "1. Forward pass: compute predictions\n",
    "2. Compute loss: compare predictions to ground truth labels\n",
    "3. Backward pass: compute gradients via backpropagation through time (BPTT)\n",
    "4. Update weights: apply optimizer step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n",
      "Batches per epoch: 5\n",
      "Model has 8,770 parameters\n",
      "\n",
      "Epoch [ 1/80] - Loss: 0.6977, Acc: 50.00%\n",
      "Epoch [ 5/80] - Loss: 0.6149, Acc: 77.50%\n",
      "Epoch [10/80] - Loss: 0.2737, Acc: 82.50%\n",
      "Epoch [15/80] - Loss: 0.1323, Acc: 92.50%\n",
      "Epoch [20/80] - Loss: 0.0591, Acc: 97.50%\n",
      "Epoch [25/80] - Loss: 0.0140, Acc: 100.00%\n",
      "Epoch [30/80] - Loss: 0.0031, Acc: 100.00%\n",
      "Epoch [35/80] - Loss: 0.0017, Acc: 100.00%\n",
      "Epoch [40/80] - Loss: 0.0013, Acc: 100.00%\n",
      "Epoch [45/80] - Loss: 0.0010, Acc: 100.00%\n",
      "Epoch [50/80] - Loss: 0.0008, Acc: 100.00%\n",
      "Epoch [55/80] - Loss: 0.0007, Acc: 100.00%\n",
      "Epoch [60/80] - Loss: 0.0006, Acc: 100.00%\n",
      "Epoch [65/80] - Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [70/80] - Loss: 0.0005, Acc: 100.00%\n",
      "Epoch [75/80] - Loss: 0.0004, Acc: 100.00%\n",
      "Epoch [80/80] - Loss: 0.0004, Acc: 100.00%\n",
      "\n",
      "Training complete!\n"
     ]
    }
   ],
   "source": [
    "# Define loss function\n",
    "# CrossEntropyLoss expects raw scores (logits) and applies softmax internally\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Define optimizer\n",
    "# Adam with slightly lower learning rate for more stable training\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Number of training epochs (reduced since we have more data)\n",
    "num_epochs = 80\n",
    "\n",
    "# Lists to store training history\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "\n",
    "print('Starting training...')\n",
    "print(f'Batches per epoch: {len(trainloader)}')\n",
    "print(f'Model has {total_params:,} parameters\\n')\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # ===== Training phase =====\n",
    "    model.train()  # Set model to training mode (enables dropout)\n",
    "    \n",
    "    epoch_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # Iterate over training batches\n",
    "    for i, (sequences, labels) in enumerate(trainloader):\n",
    "        # Move data to device\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Zero the gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass: get predictions\n",
    "        outputs = model(sequences)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward pass: compute gradients via backpropagation through time (BPTT)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate statistics\n",
    "        epoch_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    # Calculate average loss and accuracy for this epoch\n",
    "    avg_loss = epoch_loss / len(trainloader)\n",
    "    accuracy = 100 * correct / total\n",
    "    \n",
    "    # Store history\n",
    "    train_losses.append(avg_loss)\n",
    "    train_accuracies.append(accuracy)\n",
    "    \n",
    "    # Print progress every 5 epochs for cleaner output\n",
    "    if (epoch + 1) % 5 == 0 or epoch == 0:\n",
    "        print(f'Epoch [{epoch+1:2d}/{num_epochs}] - Loss: {avg_loss:.4f}, Acc: {accuracy:5.2f}%')\n",
    "\n",
    "print('\\nTraining complete!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating on Test Set\n",
    "\n",
    "Now let's evaluate our trained RNN on the test set to see how well it generalizes to unseen movie reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7080\n",
      "Test Accuracy: 80.00%\n",
      "Correct: 8/10\n"
     ]
    }
   ],
   "source": [
    "# Evaluate on test set\n",
    "model.eval()  # Set to evaluation mode\n",
    "\n",
    "test_correct = 0\n",
    "test_total = 0\n",
    "test_loss = 0.0\n",
    "\n",
    "# Store predictions for detailed analysis\n",
    "all_predictions = []\n",
    "all_labels = []\n",
    "\n",
    "# Disable gradient computation for evaluation (saves memory)\n",
    "with torch.no_grad():\n",
    "    for sequences, labels in testloader:\n",
    "        # Move to device\n",
    "        sequences = sequences.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Get predictions\n",
    "        outputs = model(sequences)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "        \n",
    "        # Get predicted class\n",
    "        _, predicted = torch.max(outputs, dim=1)\n",
    "        \n",
    "        # Store predictions and labels\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        # Count correct predictions\n",
    "        test_total += labels.size(0)\n",
    "        test_correct += (predicted == labels).sum().item()\n",
    "\n",
    "# Calculate metrics\n",
    "test_accuracy = 100 * test_correct / test_total\n",
    "avg_test_loss = test_loss / len(testloader)\n",
    "\n",
    "print(f'Test Loss: {avg_test_loss:.4f}')\n",
    "print(f'Test Accuracy: {test_accuracy:.2f}%')\n",
    "print(f'Correct: {test_correct}/{test_total}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
